{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-25T17:57:13.654245Z","iopub.execute_input":"2023-10-25T17:57:13.654589Z","iopub.status.idle":"2023-10-25T17:57:14.002457Z","shell.execute_reply.started":"2023-10-25T17:57:13.654544Z","shell.execute_reply":"2023-10-25T17:57:14.001571Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/brats-2021-task1/BraTS2021_00495.tar\n/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar\n/kaggle/input/brats-2021-task1/BraTS2021_00621.tar\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport nibabel as nib\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nfrom PIL import Image\nimport skimage.transform as skTrans","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:14.004401Z","iopub.execute_input":"2023-10-25T17:57:14.004943Z","iopub.status.idle":"2023-10-25T17:57:29.855840Z","shell.execute_reply.started":"2023-10-25T17:57:14.004890Z","shell.execute_reply":"2023-10-25T17:57:29.854832Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras import backend as K \nfrom keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:29.857153Z","iopub.execute_input":"2023-10-25T17:57:29.857837Z","iopub.status.idle":"2023-10-25T17:57:29.862388Z","shell.execute_reply.started":"2023-10-25T17:57:29.857797Z","shell.execute_reply":"2023-10-25T17:57:29.861427Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import tarfile\nextract = tarfile.open('/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar')\nextract.extractall()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:29.864578Z","iopub.execute_input":"2023-10-25T17:57:29.865198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path1 = \"/kaggle/working/BraTS2021_01154/BraTS2021_01154_t1ce.nii.gz\"\npath2 = \"/kaggle/working/BraTS2021_01154/BraTS2021_01154_t2.nii.gz\"\npath3 = \"/kaggle/working/BraTS2021_01154/BraTS2021_01154_flair.nii.gz\"\npath4 = \"/kaggle/working/BraTS2021_01154/BraTS2021_01154_seg.nii.gz\"\npath5 = \"/kaggle/working/BraTS2021_01154/BraTS2021_01154_t1.nii.gz\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Input_Images = os.listdir('/kaggle/working')[:20]\nxtrain = np.array([])\nytrain = np.array([])\ns = 0 \nfor i in Input_Images:\n    # Extracting the path of the Image\n    rel = i + \"/\" + i + '_flair.nii.gz'\n    pathinp = os.path.join('/kaggle/working',rel)\n    rel = i + \"/\" + i + '_seg.nii.gz'\n    pathseg = os.path.join('/kaggle/working',rel)\n    # Loading the image\n    Image1 = nl.image.load_img(pathinp)\n    Img_seg= nl.image.load_img(pathseg)\n    # Resizing the image\n    Image1 = skTrans.resize(Image1.get_fdata(),(64,64,64),order=1,preserve_range= True).reshape(1,64,64,64,1)\n    Img_seg = skTrans.resize(Img_seg.get_fdata(),(64,64,64),order=1,preserve_range= True).reshape(1,64,64,64,1)\n    # Stacking the data\n    if(s==0):\n        xtrain = Image1\n        ytrain = Img_seg\n        s = 1\n    else:\n        xtrain = np.vstack((xtrain,Image1))\n        ytrain = np.vstack((ytrain,Img_seg))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image1 = nl.image.load_img(path3)\nImg_seg= nl.image.load_img(path4)\nfig,axes = plt.subplots(nrows=2,figsize=(10,10))\nnlplt.plot_img(Image1,axes=axes[0])\nnlplt.plot_img(Img_seg,axes=axes[1])\nim1_in = skTrans.resize(Image1.get_fdata(),(64,64,64),order=1,preserve_range= True)\nim1_out = skTrans.resize(Img_seg.get_fdata(),(64,64,64),order=1,preserve_range= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import Model\nimport keras.backend as trace\nfrom keras.layers import Layer\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv3D,Conv3DTranspose,MaxPooling3D,Attention,GroupNormalization,Activation,Input,Concatenate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Round(x):\n    return round(x,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StocasticLayer(Layer):\n    def __init__(self,num_sample,pool_size=(2,2,2),**kwargs):\n        super(StocasticLayer,self).__init__(**kwargs)\n        self.pool_size = pool_size\n        self.num_sample = num_sample\n    def call(self,X,training=None):\n        # shape of the Image\n        batch_size,depth,height,width,n_channel  = X.shape\n#         symbolic_tensor = tf.compat.v1.placeholder(tf.float32,shape=X.shape)\n        X_numpy = X.detach().numpy()\n#         with tf.compat.v1.Session() as sees:\n#             ss1 = X\n#             X_numpy = sees.run(symbolic_tensor,feed_dict={symbolic_tensor: ss1})\n        # adjusted Size\n        out_depth = depth // self.pool_size[0]\n        out_height  = depth // self.pool_size[1]\n        out_width = depth // self.pool_size[2]\n        # reshape\n        x_reshaped = tf.reshape(X,(32,out_depth,self.pool_size[0],out_height,self.pool_size[1],out_width,self.pool_size[2],n_channel))\n        if training:\n            x_relu = trace.relu(x_reshaped)\n            x_prob = trace.softmax(x_relu)\n            X_numpy = np.maximum(0,X_numpy)\n            X_numpy = np.exp(-1 * X_numpy) /np.sum(-1 * np.exp(X_numpy))\n            index = []\n            for _ in range(self.num_sample):\n                i = tf.argmax(np.random.multinomial(1,X_numpy),size=1)\n                index.append(i)\n            out = tf.gather(x_reshaped,np.array(index),batch_dims=2)\n        else:\n            out = MaxPooling3D(pool_size=(2))(X)\n        return out\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceLoss:\n    def __init__(self,smooth,f1):\n        self.smooth = smooth\n        self.Round = f1 ;\n    def metric(self,y_true,y_pred):\n        inputs = tf.keras.layers.Flatten()(y_true)\n        targets = tf.keras.layers.Flatten()(y_pred)\n        intersection = tf.reduce_sum(tf.multiply(y_true, y_pred))\n        dice = (2*intersection ) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred))\n#         dice = tf.keras.layers.Lambda(round)(dice)\n        return 1 - dice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss Function\nimport tensorflow as tf\nclass Tversky_loss():\n    def __init__(self,al,bet):\n        self.al = al \n        self.bet = bet\n        self.const = tf.constant(np.array([1.]),dtype=tf.float32)\n    def loss(self,y_true,y_pred):\n        y_t = tf.keras.layers.Flatten()(y_true)\n        y_p = tf.keras.layers.Flatten()(y_pred)\n        Neu = tf.reduce_sum(tf.multiply(y_t,y_p)) + 1e-7\n        Deno = tf.add(tf.add(tf.reduce_sum(tf.multiply(y_t,y_p)),tf.multiply(self.al,tf.reduce_sum(tf.multiply(tf.subtract(self.const,y_t),y_p)))),\\\n                     tf.multiply(self.bet,tf.reduce_sum(tf.multiply(tf.subtract(self.const,y_p),y_t))))\n        return tf.divide(Neu,Deno)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Model_main(inp_shape,K):\n    # Block 1\n    X1 = Input(shape=inp_shape,batch_size=1)\n    X2 = Conv3D(32,K,padding='same')(X1)\n    X2 = GroupNormalization(groups=32,axis=-1)(X2)\n    X2 = Activation('relu')(X2)\n    X2 = Conv3D(32,K,padding='same')(X2)\n    X2 = GroupNormalization(groups=32,axis=-1)(X2)\n    X2 = Activation('relu')(X2)\n    X3 = MaxPooling3D(2)(X3)\n    # Block 2\n    X3 = Conv3D(64,K,padding='same')(X3)\n    X3 = GroupNormalization(groups=64,axis=-1)(X3)\n    X3 = Activation('relu')(X3)\n    X3 = Conv3D(64,K,padding='same')(X3)\n    X3 = GroupNormalization(groups=64,axis=-1)(X3)\n    X3 = Activation('relu')(X3)\n    X4 = MaxPooling3D(2)(X3)\n    # Block 3\n    X4 = Conv3D(128,K,padding='same')(X4)\n    X4 = GroupNormalization(groups=128,axis=-1)(X4)\n    X4 = Activation('relu')(X4)\n    X4 = Conv3D(128,K,padding='same')(X4)\n    X4 = GroupNormalization(groups=128,axis=-1)(X4)\n    X4 = Activation('relu')(X4)\n    X5 = MaxPooling3D(2)(X4)\n    # Block 4\n    X5 = Conv3D(256,K,padding='same')(X5)\n    X5 = GroupNormalization(groups=256,axis=-1)(X5)\n    X5 = Activation('relu')(X5)\n    X5 = Conv3D(256,K,padding='same')(X5)\n    X5 = GroupNormalization(groups=256,axis=-1)(X5)\n    X5 = Activation('relu')(X5)\n    # Block 5\n    X6 = Conv3DTranspose(128,2,2,activation='relu')(X5)\n    X7 = Attention()([X6,X4])\n    X7 = Concatenate()([X6,X7])\n    X8 = Conv3D(64,K,padding='same')(X7)\n    X8 = GroupNormalization(groups=64,axis=-1)(X8)\n    X8 = Activation('relu')(X8)\n    X8 = Conv3D(64,K,padding='same')(X7)\n    X8 = GroupNormalization(groups=64,axis=-1)(X8)\n    X8 = Activation('relu')(X8)\n    # Block 6\n    X9  = Conv3DTranspose(64,2,2,activation='relu')(X8)\n    X10 = Attention()([X9,X3])\n    X10 = Concatenate()([X9,X10])\n    X11 = Conv3D(32,K,padding='same')(X10)\n    X11 = GroupNormalization(groups=32,axis=-1)(X11)\n    X11 = Activation('relu')(X11)\n    X11 = Conv3D(32,K,padding='same')(X11)\n    X11 = GroupNormalization(groups=32,axis=-1)(X11)\n    X11 = Activation('relu')(X11)\n    # Block 7\n    X12 = Conv3DTranspose(32,2,2,activation='relu')(X11)\n    X13 = Attention()([X12,X2])\n    X13 = Concatenate()([X12,X13])\n    model = Model(X1,X13)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Branch():\n    model2 = Sequential()\n    model2.add(Conv3D(16,3,padding='same',input_shape=(64, 64, 64, 64),batch_size=1))\n    model2.add(GroupNormalization(groups=16,axis=-1))\n    model2.add(Activation('relu'))\n    model2.add(Conv3D(16,3,padding='same',input_shape=(64, 64, 64, 64)))\n    model2.add(GroupNormalization(groups=16,axis=-1))\n    model2.add(Activation('relu'))\n    model2.add(Conv3D(1,20,1,padding='same'))\n#     model2.compile(optimizer='adam',loss=Loss.loss,metrics=metrics.metric)\n    return model2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(learning_rate=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Main = Model_main((64,64,64,1),3)\nBranch_mean = Branch()\nmodel_mean_main  = Sequential([Main,Branch_mean])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Loss_mean = Tversky_loss(al=0.5,bet=0.5)\nmetric=DiceLoss(1e-6,Round)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_mean_main.compile(optimizer=opt,loss=Loss_mean.loss,metrics=metric.metric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_mean_main.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_mean = model_mean_main.fit(xtrain,ytrain,epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layers in Main.layers:\n    layers.trainable = 'false'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Branch_lower = Branch()\nmodel_lower_main  = Sequential([Main,Branch_mean])\nLoss_lower = Tversky_loss(al=0.2,bet=0.8)\nmodel_lower_main.compile(optimizer=opt,loss=Loss_lower.loss,metrics=metric.metric)\nhistory_lower = model_lower_main.fit(xtrain,ytrain,epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Branch_upper = Branch()\nmodel_upper_main  = Sequential([Main,Branch_lower])\nLoss_upper = Tversky_loss(al=0.8,bet=0.2)\nmodel_upper_main.compile(optimizer=opt,loss=Loss_upper.loss,metrics=metric.metric)\nhistory_upper = model_upper_main.fit(xtrain,ytrain,epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Mean Loss\")\nplt.plot(history_mean.history['loss'][1:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Lower Loss\")\nplt.plot(history_lower.history['loss'][1:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Upper Loss\")\nplt.plot(history_upper.history['loss'][1:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}